{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with(open('./data/driving_log.csv')) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        #if line.split(\"/\")[1].split(\"_\")[0] == 'center':\n",
    "        lines.append(line)\n",
    "\n",
    "# Removing the column title row\n",
    "lines = lines[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating a blank dictionary for data\n",
    "data = {'train':[], 'validation':[]}\n",
    "for line in lines:\n",
    "    # Reading the steering measurement\n",
    "    measurement = float(line[3])\n",
    "    # Removing about 95% of the images with measurement = 0\n",
    "    if measurement != 0 or np.random.rand() > 0.95:\n",
    "        # loop 3 times for three different positions of left, center, right\n",
    "        for i in range(3):\n",
    "            datapoint = {}\n",
    "            datapoint['path'] = './data/' + line[i].lstrip()\n",
    "            datapoint['position'] = line[i].split('/')[1].split('_')[0]\n",
    "            datapoint['flipped'] = False\n",
    "            # Adding or subtracting 5.0 (can be tuned) to the steering angle based on the camera position\n",
    "            if datapoint['position'] == 'right':                \n",
    "                datapoint['measurement'] = measurement - 5.0\n",
    "            elif datapoint['position'] == 'left':\n",
    "                datapoint['measurement'] = measurement + 5.0\n",
    "            else:\n",
    "                datapoint['measurement'] = measurement\n",
    "            if datapoint['position'] == 'center' and np.random.rand() > 0.8:\n",
    "                data['validation'].append(datapoint)\n",
    "            else:\n",
    "                data['train'].append(datapoint)\n",
    "            # Creating a copy of image dictionary to avoid pointing to the previous one\n",
    "            datapoint = dict(datapoint)    \n",
    "            # Augmenting with the flipped image\n",
    "            datapoint['flipped'] = True\n",
    "            datapoint['measurement'] = -datapoint['measurement']\n",
    "            data['train'].append(datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, samples, batch_size=128, dim=(160,320), n_channels=3, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.samples = samples\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.samples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_datapoints_temp = [self.samples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_datapoints_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.samples))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_datapoints_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, datapoint in enumerate(list_datapoints_temp):\n",
    "            image = plt.imread(datapoint['path'])\n",
    "            # flipping horizontal if necessary\n",
    "            if datapoint['flipped']:\n",
    "                image = cv2.flip(image, 1)\n",
    "            # Store sample\n",
    "            X[i,] = image\n",
    "\n",
    "            # Store value\n",
    "            y[i] = datapoint['measurement']\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (160,320), \n",
    "          'batch_size': 128, \n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(data['train'], **params)\n",
    "validation_generator = DataGenerator(data['validation'], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 320, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator.__getitem__(0)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(images):\n",
    "    images = tf.image.rgb_to_yuv(images)\n",
    "    images = tf.image.crop_to_bounding_box(images, 60, 0, 80, 320)\n",
    "    images = tf.image.resize(images, [66,200])\n",
    "    images = tf.math.add(tf.math.divide(images,128), -1)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Architecture\n",
    "## Setup Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Cropping2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building DAVE2 Architecture in Keras \n",
    "model = Sequential()\n",
    "\n",
    "# preprocessing\n",
    "#model.add(Lambda(image_preprocess, input_shape=(160, 320, 3)))\n",
    "model.add(Lambda(lambda x: x/128.0 - 1.0, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((60,20),(0,0))))\n",
    "\n",
    "model.add(Conv2D(24, (5,5), strides=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(36, (5,5), strides=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (5,5), strides=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 80, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 38, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38, 158, 24)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 77, 36)        21636     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 17, 77, 36)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 37, 48)         43248     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 37, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 35, 64)         27712     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5, 35, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 33, 64)         36928     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3, 33, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6336)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               633700    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 770,619\n",
      "Trainable params: 770,619\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sbonab\\miniconda3\\envs\\carnd-term1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "176/176 [==============================] - 106s 604ms/step - loss: 15.9619 - val_loss: 0.8316\n",
      "Epoch 2/10\n",
      "176/176 [==============================] - 103s 587ms/step - loss: 15.0153 - val_loss: 0.9124\n",
      "Epoch 3/10\n",
      "176/176 [==============================] - 101s 575ms/step - loss: 14.1946 - val_loss: 0.9088\n",
      "Epoch 4/10\n",
      "176/176 [==============================] - 105s 597ms/step - loss: 14.1767 - val_loss: 0.8557\n",
      "Epoch 5/10\n",
      "176/176 [==============================] - 107s 609ms/step - loss: 13.9075 - val_loss: 0.8806\n",
      "Epoch 6/10\n",
      "176/176 [==============================] - 101s 576ms/step - loss: 13.9177 - val_loss: 0.8586\n",
      "Epoch 7/10\n",
      "176/176 [==============================] - 102s 578ms/step - loss: 13.7414 - val_loss: 0.8846\n",
      "Epoch 8/10\n",
      "176/176 [==============================] - 97s 549ms/step - loss: 13.6315 - val_loss: 0.9005\n",
      "Epoch 9/10\n",
      "176/176 [==============================] - 93s 527ms/step - loss: 13.7836 - val_loss: 0.8978\n",
      "Epoch 10/10\n",
      "176/176 [==============================] - 94s 532ms/step - loss: 13.7682 - val_loss: 0.8826\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator, \n",
    "                    epochs=10,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
